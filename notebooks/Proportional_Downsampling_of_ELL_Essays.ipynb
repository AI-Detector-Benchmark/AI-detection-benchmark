{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This notebook performs proportional downsampling of ELL essays\n",
        "based on aggregated writing quality scores.\n",
        "\n",
        "The goal is to obtain a smaller but balanced subset for downstream analysis.\n"
      ],
      "metadata": {
        "id": "ryfhIbGIRq9y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b17VVHgwQPlf",
        "outputId": "e542ccd0-4115-4393-bb47-626b749b7439"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking Google Drive connection...\n",
            "Mounted at /content/drive\n",
            "Drive mounted successfully.\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas -q\n",
        "\n",
        "import pandas as pd\n",
        "import json\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# 1. GOOGLE DRIVE CONNECTION\n",
        "print(\"Checking Google Drive connection...\")\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "    print(\"Drive mounted successfully.\")\n",
        "else:\n",
        "    print(\"Drive is already mounted.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "INPUT_FILE_PATH = '/content/drive/MyDrive/Fırat Projeler/Detecting AI Influence in Student Writing: Toward Reliable and Interpretable Classifiers/feedback-prize-english-language-learning/ell_essay_families_structure_V2.jsonl'\n",
        "OUTPUT_SAMPLED_PATH = '/content/drive/MyDrive/Fırat Projeler/Detecting AI Influence in Student Writing: Toward Reliable and Interpretable Classifiers/feedback-prize-english-language-learning/ell_balanced_1K.jsonl'\n",
        "\n",
        "# How many samples will be taken from each group and the total number of texts\n",
        "TARGET_TOTAL = 1000\n",
        "TARGET_PER_GROUP = 250\n",
        "\n",
        "\n",
        "data = []\n",
        "if os.path.exists(INPUT_FILE_PATH):\n",
        "    print(f\"Reading file: {INPUT_FILE_PATH}\")\n",
        "    with open(INPUT_FILE_PATH, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            data.append(json.loads(line))\n",
        "    print(f\"Total Original Dataset Size: {len(data)}\")\n",
        "else:\n",
        "    print(f\"ERROR: File not found! Please check the path:\\n{INPUT_FILE_PATH}\")\n",
        "    data = []\n",
        "\n",
        "if data:\n",
        "    # scoring\n",
        "    df_rows = []\n",
        "    for item in data:\n",
        "        scores = item['source']['original_scores']\n",
        "\n",
        "        # Average of Grammar, Vocabulary, Cohesion, and Syntax scores\n",
        "        # Phraseology and Conventions are excluded\n",
        "        quality_score = (scores['grammar'] + scores['vocabulary'] + scores['cohesion'] + scores['syntax']) / 4\n",
        "\n",
        "        # 4 groups\n",
        "        if quality_score < 2.5:\n",
        "            group = '1_Low (1.0-2.5)'\n",
        "        elif quality_score < 3.0:\n",
        "            group = '2_MidLow (2.5-3.0)'\n",
        "        elif quality_score < 3.5:\n",
        "            group = '3_MidHigh (3.0-3.5)'\n",
        "        else:\n",
        "            group = '4_High (3.5-5.0)'\n",
        "\n",
        "        df_rows.append({\n",
        "            'json_data': item,\n",
        "            'score': quality_score,\n",
        "            'group': group\n",
        "        })\n",
        "\n",
        "    df = pd.DataFrame(df_rows)\n",
        "\n",
        "    print(\"\\n--- Original Data Distribution ---\")\n",
        "    print(df['group'].value_counts().sort_index())\n",
        "\n",
        "    # Sampling step\n",
        "    sampled_dfs = []\n",
        "    groups = df['group'].unique()\n",
        "\n",
        "    for g in groups:\n",
        "        subset = df[df['group'] == g]\n",
        "\n",
        "        # If there is not enough data in the group, take all of them\n",
        "        if len(subset) < TARGET_PER_GROUP:\n",
        "            print(f\"Warning: Group '{g}' contains only {len(subset)} samples. Taking all of them.\")\n",
        "            sampled = subset\n",
        "        else:\n",
        "            # If more than 250 samples, select randomly\n",
        "            sampled = subset.sample(n=TARGET_PER_GROUP, random_state=42)\n",
        "\n",
        "        sampled_dfs.append(sampled)\n",
        "\n",
        "    final_df = pd.concat(sampled_dfs)\n",
        "\n",
        "    # If some groups have fewer texts and the total is still insufficient, randomly sample the remaining ones\n",
        "    if len(final_df) < TARGET_TOTAL:\n",
        "        needed = TARGET_TOTAL - len(final_df)\n",
        "        print(f\"\\nAdding {needed} random samples from the remaining pool to reach 1000.\")\n",
        "\n",
        "        remaining = df.drop(final_df.index)  # Remove already selected samples from the pool\n",
        "        extra = remaining.sample(n=needed, random_state=42)\n",
        "        final_df = pd.concat([final_df, extra])\n",
        "\n",
        "    # 7. DISPLAY AND SAVE RESULTS\n",
        "    print(\"\\n--- Selected (Balanced) Distribution ---\")\n",
        "    print(final_df['group'].value_counts().sort_index())\n",
        "    print(f\"Total Selected Samples: {len(final_df)}\")\n",
        "\n",
        "    # Write to file\n",
        "    with open(OUTPUT_SAMPLED_PATH, 'w', encoding='utf-8') as f:\n",
        "        for item in final_df['json_data']:\n",
        "            f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
        "\n",
        "    print(f\"\\n✅ SUCCESS: '1000 text samples have been saved to:\\n{OUTPUT_SAMPLED_PATH}\")\n",
        "\n",
        "else:\n",
        "    print(\"Processing could not be performed because the data could not be loaded.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sx-DyGeFQTzQ",
        "outputId": "7c71764d-7c6d-44b8-d682-3a6741de03b5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading file: /content/drive/MyDrive/Fırat Projeler/Detecting AI Influence in Student Writing: Toward Reliable and Interpretable Classifiers/feedback-prize-english-language-learning/ell_essay_families_structure_V2.jsonl\n",
            "Total Original Dataset Size: 3911\n",
            "\n",
            "--- Original Data Distribution ---\n",
            "group\n",
            "1_Low (1.0-2.5)         441\n",
            "2_MidLow (2.5-3.0)     1137\n",
            "3_MidHigh (3.0-3.5)    1240\n",
            "4_High (3.5-5.0)       1093\n",
            "Name: count, dtype: int64\n",
            "\n",
            "--- Selected (Balanced) Distribution ---\n",
            "group\n",
            "1_Low (1.0-2.5)        250\n",
            "2_MidLow (2.5-3.0)     250\n",
            "3_MidHigh (3.0-3.5)    250\n",
            "4_High (3.5-5.0)       250\n",
            "Name: count, dtype: int64\n",
            "Total Selected Samples: 1000\n",
            "\n",
            "✅ SUCCESS: '1000 text samples have been saved to:\n",
            "/content/drive/MyDrive/Fırat Projeler/Detecting AI Influence in Student Writing: Toward Reliable and Interpretable Classifiers/feedback-prize-english-language-learning/ell_balanced_1K.jsonl\n"
          ]
        }
      ]
    }
  ]
}